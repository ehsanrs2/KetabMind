QDRANT_MODE=local
QDRANT_LOCATION=./qdrant_local
QDRANT_COLLECTION=books
# EMBED_MODEL options: mock | small | base
EMBED_MODEL=small
# LLM configuration
# LLM_BACKEND options: mock | ollama | transformers
LLM_BACKEND=ollama
LLM_MODEL=mixtral:8x7b-instruct-v0.1-q3_K_M 
OLLAMA_HOST=http://localhost:11434
LLM_MAX_INPUT_TOKENS=4096
LLM_MAX_NEW_TOKENS=256
LLM_TEMPERATURE=0.2
LLM_TOP_P=0.95
LLM_DEVICE=cuda
LLM_LOAD_IN_4BIT=true
CHUNK_SIZE=800
CHUNK_OVERLAP=200

# Example Ollama configuration
# OLLAMA_HOST=http://localhost:11434
# LLM_BACKEND=ollama
# LLM_MODEL=mistral:7b-instruct-q4_K_M
# LLM_MAX_NEW_TOKENS=256
# Note: for GPU backend see next steps.

# Transformers GPU setup (RTX 3060 reference)
# poetry install --with gpu
# python -c "import torch; print(torch.cuda.is_available())"
# LLM_BACKEND=transformers
# LLM_MODEL=microsoft/Phi-3-mini-4k-instruct
# Alternate: mistralai/Mistral-7B-Instruct (set LLM_LOAD_IN_4BIT=true)
CHUNK_SIZE=800
CHUNK_OVERLAP=200
